
\subsection{Runtime}\label{ssec:runtime}
We can now turn to the question of runtime, think of a programming language as giving us a number of basic operations, where each basic operation has some cost assigned to it. We then obtain an abstract notion of cost/runtime of a program on input $x$ simply by adding the cost of the steps that the program makes on $x$. This idea abstracts away from the notion of \emph{actual} time which we measure in seconds and not in steps. 

In many programming settings, we prefer to measure in seconds (or preferably microseconds), and we often try out various optimizations and aim to reduce the time our actual system takes. But in cryptography, we need to reason about large-scale computation, namely attacks which might take an exponential number of steps in the length of a key. Here, the actual impact of the parameters of a system on the computation time becomes much less important, since the system parameters are usually some constant factor whereas the number of steps captures the more important aspect of the computation. Additionally, in cryptography, we cannot experimentally determine how long an attack takes, because secure cryptography should be such that no one can perform an attack, not even we ourselves. Thus, it becomes crucial to \emph{reason} about runtime of algorithms because we cannot experimentally test it.

\paragraph{Polynomial-Time} If our system uses a key of length $\lambda$, we are interested in the runtime of the best attack \emph{as a function of} $\lambda$. If an attack takes a \emph{polynomial} number of steps (polynomial in $\lambda$) and is quite likely to be successful, we consider the cryptosystem \emph{insecure}. A word is in order of why we consider polynomial-time and what the advantages and disadvantages of this choice are. We consider polynomial-time, mostly, because it is \emph{convenient} for two reasons. 

Firstly, the Cobham-Edmonds Thesis~\footnote{See, e.g., Section 1.3.5 in \url{http://www.wisdom.weizmann.ac.il/~oded/CC/bc-1.pdf} for a longer discussion of the thesis, including its appeal to the intuitive notion of a \emph{reasoneable} model.}, a strong version of the Church-Turing Thesis, states that all \emph{reasonable} models of computation are polynomially related. I.e., it does not exactly matter what we define as \emph{1 step}, since any \emph{reasonable} interpretation of \emph{1 step} will lead to the same definition of polynomial-time. This means that in this course, we can argue about steps intuitively, and for each exercise and theorem, we can say \emph{I count this as 1 step and this as 1 step, and then my overall computation time is ... which is a polynomial in $\lambda$}, using different definitions of step in different exercises/theorems.

Secondly, we do not need to refer to parameters in security statements. Instead of saying ``$t$-secure'' or ``secure against algorithms running it time at most $t$'', we can simply use the term \emph{secure} which implicitly refers to polynomial-time.

One disadvantage of using polynomial-time is that it is quite coarse and does not make interesting statements about the security for a concrete $\lambda$. E.g., $\lambda^2+2^{128}$ is a polynomial in $\lambda$, since $2^{128}$ is a (very large) constant (independent of $\lambda$). Also, $\lambda^2$ and $\lambda^{100}$ are both polynomials in $\lambda$, but a cryptosystem which is secure against algorithms running for $\lambda^{100}$ steps could potentially already be useful in practice. These latter artefacts about considering polynomial-time are unfortunate and insightful at the same time. They are unfortunate, because they reveal that proper engineering investigation needs to be carried out before deploying theoretically sound cryptography in practice. Indeed, non-surprisingly, there are quite a number of engineering considerations which are necessary for security in practice. We return to the matter of engineering choices for cryptography versus the end of the course.
 On the other hand, it is insightful to understand whether the slow runtime of an (attack) algorithm stems from a large constant or whether the attack grows more inefficient when the key length grows. In the latter case, the design principle is sound even when computers get much faster---we can simply increase our key length and yield an cryptographic algorithms which is secure w.r.t. current state-of-the-art computation. Last, but not least, a nice feature of polynomials is that they are closed under addition, multiplication and subsequent evaluation, i.e., when $q(\lambda)$ and $p(\lambda)$ are polynomials in $\lambda$, then $q(\lambda)+p(\lambda)$, $q(\lambda)\cdot p(\lambda)$ and $q(p(\lambda))$ are also polynomials in $\lambda$ which is useful to argue about the runtime of composed algorithms, a matter of central importance in cryptography, as we discuss next.

\paragraph{Security parameter} In most cryptographic systems, we have a key, and we can think of the keylength as a \emph{security parameter}.
However, some cryptographic primitives do not have a key, e.g., one-way functions. In order to allow for a uniform treatment of keyed and unkeyed primitives, we simply add an explicit \emph{security parameter} to our system which we write as $1^\lambda$, i.e., a bitstring consisting of $\lambda$ ones. Our cryptosystems and adversaries then also get the security parameter as input, and we will argue about all adversaries running in time at most polynomial in $\lambda$. The encoding of $\lambda$ via $1^\lambda$ comes from a tradition of complexity theory where runtime of an algorithm is defined as a function of the length of its input. Using an explicit parameter $\lambda$ is more convenient (since we do not need to bother with encoding information into the length of a value), and encoding the value as $1^\lambda$ is simply to keep the encoding compatible with the historical attitude of complexity theory. See Section~\ref{sssec:security-parameter} for more details on the role of the security parameter in modeling of cryptographic security.